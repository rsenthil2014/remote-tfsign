{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CNN based Traffic Sign Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Configuration parameters\n",
    "IMG_SIZE = 32 # IMG_SIZE x IMG_SIZE\n",
    "GRAYSCALE = False # convert image to gray scale?\n",
    "NUM_CHANNELS = 1 if GRAYSCALE else 3\n",
    "NUM_CLASSES = 43\n",
    "\n",
    "#Model parameters\n",
    "LR = 5e-3 # Learning rate\n",
    "KEEP_PROB = 0.5 #drop out for training\n",
    "OPT = tf.train.GradientDescentOptimizer(learning_rate = LR)\n",
    "\n",
    "#Training process\n",
    "RESTORE = False\n",
    "RESUME = False\n",
    "NUM_EPOCH =40\n",
    "BATCH_SIZE = 128\n",
    "BATCH_SIZE_INF = 2048 # For calculating accuracy\n",
    "VALIDATION_SIZE = 0.2 # fraction to be used as validation set\n",
    "SAVE_MODEL = True # To save trained model to disk\n",
    "MODEL_SAVE_PATH = 'C:/git/gtk/Traffic-sign-Recognition/model.ckpt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###########################################\n",
    "## Helper functions\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "#training_file = 'c:/tmp/GTS/train.p'\n",
    "training_file = 'train.p'\n",
    "testing_file = 'test.p'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "    \n",
    "\n",
    "def rgb2gray(rgb):    \n",
    "    # Convert RGB images to Grayscale\n",
    "    return np.dot(rgb[...,:3],[0.299,0.587,0.114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X,y):\n",
    "    # preprocess data \n",
    "    if GRAYSCALE:\n",
    "        X = rgb2gray(X)\n",
    "\n",
    "    # Make all image values fall with in range of -1 to 1\n",
    "    X= X.astype('float32')\n",
    "    X = (X-128.)/128.\n",
    "\n",
    "    #convert the labels from numeric to one hot encoded\n",
    "    y_onehot = np.zeros((y.shape[0], NUM_CLASSES))\n",
    "    for i, onehot_label in enumerate(y_onehot):\n",
    "        onehot_label[y[i]]=1.\n",
    "    y = y_onehot\n",
    "\n",
    "    return X,y\n",
    "\n",
    "def next_batch(X, y, batch_size, augment_data):\n",
    "\n",
    "    #provide data batch wise\n",
    "    start_idx = 0\n",
    "    while start_idx < X.shape[0]:\n",
    "        images = X[start_idx : start_idx + batch_size]\n",
    "        labels = y[start_idx : start_idx + batch_size]\n",
    "        yield(np.array(images), np.array(labels))\n",
    "        # Yield will make sure the continuty of the batch elements \n",
    "        start_idx += batch_size\n",
    "\n",
    "def calculate_accuracy(data_gen, data_size, batch_size, accuracy, x,y, keep_prob, sess):\n",
    "\n",
    "    num_batches = math.ceil(data_size/ batch_size)\n",
    "    last_batch_size = data_size % batch_size\n",
    "\n",
    "    accs = [] # accuracy for each batch\n",
    "    for _ in range(num_batches):\n",
    "        images,labels = next(data_gen)\n",
    "\n",
    "        #Keep probability to 1 as it is inference\n",
    "        acc = sess.run(accuracy,feed_dict = {x:images, y:labels, keep_prob:1.})\n",
    "        accs.append(acc)\n",
    "    # average of all full batches, except last batch\n",
    "    acc_full = np.mean(accs[:-1])\n",
    "\n",
    "    acc = (acc_full *(data_size - last_batch_size)+accs[-1] * last_batch_size)/data_size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###########################################\n",
    "## Conv Neural Network functions\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def neural_network():\n",
    "    ''' Define the CNN network'''\n",
    "    \n",
    "    \n",
    "    #Tensor representing input images and labels\n",
    "    x = tf.placeholder(tf.float32, [None,IMG_SIZE,IMG_SIZE,NUM_CHANNELS])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    y_ = tf.placeholder(tf.float32, [None , NUM_CLASSES])\n",
    "\n",
    "    # Reshape input picture\n",
    "    #x = tf.reshape(x, shape=[-1,IMG_SIZE,IMG_SIZE,NUM_CHANNELS])\n",
    "\n",
    "    # First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "    W_conv1 = weight_variable([5, 5, NUM_CHANNELS, 16])\n",
    "    b_conv1 = bias_variable([16])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "    # Pooling layer - downsamples by 2X.\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    # Second convolutional layer -- maps 16 feature to 32 feature.\n",
    "    W_conv2 = weight_variable([5, 5, 16, 32])\n",
    "    b_conv2 = bias_variable([32])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    # Second pooling layer.\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # Third convolutional layer -- maps 32 feature to 64 feature.\n",
    "    W_conv3 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv3 = bias_variable([64])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "    # Third pooling layer.\n",
    "    h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "    # Fully connected layer 1 - 4x4x64 feature maps -- maps this to 1024 features.\n",
    "    W_fc1 = weight_variable([4 * 4 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    #x =           tf.reshape(x, shape=[-1,IMG_SIZE,IMG_SIZE,NUM_CHANNELS])\n",
    "    h_pool3_flat = tf.reshape(h_pool3, shape=[-1, 4*4*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Dropout - controls the complexity of the model, prevents co-adaptation of features.\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    # Map the 1024 features to 43 classes, one for each class\n",
    "    W_fc2 = weight_variable([1024, NUM_CLASSES])\n",
    "    b_fc2 = bias_variable([NUM_CLASSES])    \n",
    "\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    #loss = tf.reduce_mean(\n",
    "    #  tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y_conv))\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits=y_conv,labels=y_))\n",
    "\n",
    "    logits = y_conv\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    prediction = tf.argmax(y_conv,1)\n",
    "\n",
    "    correct_prediction = tf.equal(prediction, tf.argmax(y_, 1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return x,y_,keep_prob, logits,optimizer,prediction,accuracy    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###########################################\n",
    "### Train Conv Neural Network \n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model from scratch\n",
      "Epoch 1 -- Train acc.:0.3349, Validation acc.:0.3315, Elapsed time: 165.80 sec\n",
      "Trained model saved at: C:/git/gtk/Traffic-sign-Recognition/model.ckpt\n",
      "Accuracy history saved at accuracy_history.p\n",
      "Epoch 10 -- Train acc.:0.9356, Validation acc.:0.9134, Elapsed time: 1505.60 sec\n",
      "Trained model saved at: C:/git/gtk/Traffic-sign-Recognition/model.ckpt\n",
      "Accuracy history saved at accuracy_history.p\n",
      "Epoch 20 -- Train acc.:0.9871, Validation acc.:0.9685, Elapsed time: 1685.16 sec\n",
      "Trained model saved at: C:/git/gtk/Traffic-sign-Recognition/model.ckpt\n",
      "Accuracy history saved at accuracy_history.p\n",
      "Epoch 30 -- Train acc.:0.9971, Validation acc.:0.9832, Elapsed time: 1676.37 sec\n",
      "Trained model saved at: C:/git/gtk/Traffic-sign-Recognition/model.ckpt\n",
      "Accuracy history saved at accuracy_history.p\n",
      "Epoch 40 -- Train acc.:0.9992, Validation acc.:0.9883, Elapsed time: 1671.01 sec\n",
      "Trained model saved at: C:/git/gtk/Traffic-sign-Recognition/model.ckpt\n",
      "Accuracy history saved at accuracy_history.p\n",
      "Total elapsed time: 6704.62 sec (111.74 min)\n",
      "Calculating test accuracy...\n",
      "Test acc.: 0.8865\n",
      "Trained model saved at: C:/git/gtk/Traffic-sign-Recognition/model.ckpt\n",
      "Accuracy history saved at accuracy_history.p\n"
     ]
    }
   ],
   "source": [
    "def train_network():\n",
    "\n",
    "\n",
    "    #load_data()\n",
    "    with open(training_file, mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "\n",
    "    with open(testing_file, mode='rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    \n",
    "    X_train, y_train = train['features'], train['labels']\n",
    "    X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "    X_train, y_train = preprocess_data(X_train, y_train)\n",
    "    X_test, y_test = preprocess_data(X_test, y_test)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=VALIDATION_SIZE)\n",
    "\n",
    "    # Launching Graph\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        x,y, keep_prob,logits, optimizer, prediction, accuracy = neural_network()\n",
    "\n",
    "        # Begin Training\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        # Dump summary for Tensor Board\n",
    "        train_writer = tf.summary.FileWriter('tf_summary/train', sess.graph)\n",
    "        \n",
    "        if RESUME or RESTORE:\n",
    "            print('Restoring previously trained model at %s'% MODEL_SAVE_PATH)\n",
    "            #Restore previously trained model\n",
    "            saver.restore(sess, MODEL_SAVE_PATH)\n",
    "\n",
    "            #Restore previous accuracy history\n",
    "            with open('accuracy_history.p','rb')as f:\n",
    "                accuracy_history = pickle.load(f)\n",
    "\n",
    "            if RESTORE:\n",
    "                return accuracy_history\n",
    "        else:\n",
    "            print('Training model from scratch')\n",
    "            #init = tf.initialize_all_variables()\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "\n",
    "            #keep track of training and validation accuracy over EPOCH \n",
    "            accuracy_history = []\n",
    "\n",
    "        #Record time elapsed for performance check\n",
    "        last_time = time.time()\n",
    "        train_start_time = time.time()\n",
    "\n",
    "        #Run NUM_EPOCH epochs of training\n",
    "        for epoch in range(NUM_EPOCH):\n",
    "\n",
    "            train_gen = next_batch(X_train, y_train, BATCH_SIZE, True)\n",
    "\n",
    "            num_batches_train = math.ceil(X_train.shape[0]/BATCH_SIZE)\n",
    "\n",
    "            # Run training on each batch\n",
    "            for _ in range(num_batches_train):\n",
    "\n",
    "                images, labels = next(train_gen)\n",
    "\n",
    "                #perform gradient update in current batch\n",
    "                sess.run(optimizer, feed_dict={x:images,y:labels, keep_prob:KEEP_PROB})\n",
    "\n",
    "            #Training set\n",
    "            train_gen = next_batch(X_train, y_train, BATCH_SIZE_INF, True)\n",
    "            train_size = X_train.shape[0]\n",
    "            train_acc = calculate_accuracy(train_gen, train_size, BATCH_SIZE_INF, accuracy, x, y, keep_prob, sess)\n",
    "\n",
    "            # Validation set\n",
    "            valid_gen = next_batch(X_valid, y_valid, BATCH_SIZE_INF, True)\n",
    "            valid_size = X_valid.shape[0]\n",
    "            valid_acc = calculate_accuracy(valid_gen, valid_size, BATCH_SIZE_INF, accuracy, x, y, keep_prob, sess)\n",
    "\n",
    "            #record accuracy for report\n",
    "            accuracy_history.append((train_acc, valid_acc))\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Print accuracy every 10 epochs\n",
    "            if(epoch+1)%10 == 0 or epoch ==0 or (epoch+1) == NUM_EPOCH:\n",
    "                print('Epoch %d -- Train acc.:%.4f, Validation acc.:%.4f, Elapsed time: %.2f sec' %\\\n",
    "                     (epoch+1, train_acc, valid_acc, time.time() - last_time))\n",
    "                last_time = time.time()\n",
    "\n",
    "                if SAVE_MODEL:\n",
    "                    # Save model to disk- check point for every 10 EPOCH\n",
    "                    save_path = saver.save(sess, MODEL_SAVE_PATH)\n",
    "                    print('Trained model saved at: %s' % save_path)\n",
    "\n",
    "                    # Also save accuracy history\n",
    "                    print('Accuracy history saved at accuracy_history.p')\n",
    "                    with open('accuracy_history.p', 'wb') as f:\n",
    "                        pickle.dump(accuracy_history, f)\n",
    "            \n",
    "\n",
    "        total_time = time.time() - train_start_time\n",
    "        print('Total elapsed time: %.2f sec (%.2f min)' % (total_time, total_time/60))          \n",
    "\n",
    "        # After training is complete, evaluate accuracy on test set\n",
    "        print('Calculating test accuracy...')\n",
    "        test_gen = next_batch(X_test, y_test, BATCH_SIZE_INF, False)\n",
    "        test_size = X_test.shape[0]\n",
    "        test_acc = calculate_accuracy(test_gen, test_size, BATCH_SIZE_INF, accuracy, x, y, keep_prob, sess)\n",
    "        print('Test acc.: %.4f' % (test_acc,))\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            # Save model to disk\n",
    "            save_path = saver.save(sess, MODEL_SAVE_PATH)\n",
    "            print('Trained model saved at: %s' % save_path)\n",
    "\n",
    "            # Also save accuracy history\n",
    "            print('Accuracy history saved at accuracy_history.p')\n",
    "            with open('accuracy_history.p', 'wb') as f:\n",
    "                pickle.dump(accuracy_history, f)\n",
    "    return accuracy_history\n",
    "                \n",
    "accuracy_history = train_network()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Report run from local folder\n",
    "Training model from scratch\n",
    "Epoch 1 -- Train acc.:0.3654, Validation acc.:0.3518, Elapsed time: 169.46 sec\n",
    "Trained model saved at: c:/tmp/GTS/model.ckpt\n",
    "Accuracy history saved at accuracy_history.p\n",
    "Epoch 10 -- Train acc.:0.9372, Validation acc.:0.9156, Elapsed time: 1564.11 sec\n",
    "Trained model saved at: c:/tmp/GTS/model.ckpt\n",
    "Accuracy history saved at accuracy_history.p\n",
    "Epoch 20 -- Train acc.:0.9876, Validation acc.:0.9690, Elapsed time: 1739.42 sec\n",
    "Trained model saved at: c:/tmp/GTS/model.ckpt\n",
    "Accuracy history saved at accuracy_history.p\n",
    "Epoch 30 -- Train acc.:0.9970, Validation acc.:0.9829, Elapsed time: 1671.16 sec\n",
    "Trained model saved at: c:/tmp/GTS/model.ckpt\n",
    "Accuracy history saved at accuracy_history.p\n",
    "Epoch 40 -- Train acc.:0.9991, Validation acc.:0.9890, Elapsed time: 1676.31 sec\n",
    "Trained model saved at: c:/tmp/GTS/model.ckpt\n",
    "Accuracy history saved at accuracy_history.p\n",
    "Total elapsed time: 6821.04 sec (113.68 min)\n",
    "Calculating test accuracy...\n",
    "Test acc.: 0.8857\n",
    "Trained model saved at: c:/tmp/GTS/model.ckpt\n",
    "Accuracy history saved at accuracy_history.p\n",
    "In [ ]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
